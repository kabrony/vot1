#!/usr/bin/env python3
"""
GitHub Update Automation

This script extends the GitHub Ecosystem Analyzer with automation capabilities for making updates to repositories.
It can automatically create pull requests for suggested improvements, update documentation, and keep repositories
synchronized based on the improvement plans generated by the analyzer.

Key features:
1. Automatic pull request creation for suggested improvements
2. Documentation updates based on analysis findings
3. Dependency updates and security fixes
4. Repository standardization according to best practices
5. Workflow optimization and CI/CD enhancements
6. Utilizes MCP Composio GitHub integration for enhanced performance and capabilities
"""

import os
import sys
import argparse
import logging
import json
import asyncio
import time
from typing import Dict, List, Any, Optional, Union, Tuple
from pathlib import Path

# Add the src directory to the path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.vot1.vot_mcp import VotModelControlProtocol
from src.vot1.memory import MemoryManager
from src.vot1.github_app_bridge import GitHubAppBridge
from src.vot1.github_composio_bridge import GitHubComposioBridge
from scripts.github_ecosystem_analyzer import GitHubEcosystemAnalyzer
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join('logs', 'github_update_automation.log'), mode='a')
    ]
)
logger = logging.getLogger(__name__)

class GitHubUpdateAutomation:
    """
    Automated GitHub repository updates based on ecosystem analysis.
    
    This class provides automation for implementing improvements identified by the
    GitHub Ecosystem Analyzer, creating pull requests, updating documentation, and
    ensuring repositories follow best practices.
    
    Uses MCP Composio GitHub integration for enhanced capabilities and performance.
    """
    
    def __init__(
        self,
        analyzer: Optional[GitHubEcosystemAnalyzer] = None,
        primary_model: str = "anthropic/claude-3-7-sonnet-20240620",
        secondary_model: str = "anthropic/claude-3-5-sonnet-20240620",
        use_extended_thinking: bool = True,
        max_thinking_tokens: int = 16000,  # Increased for maximum thinking
        memory_path: Optional[str] = None,
        github_token: Optional[str] = None,
        composio_api_key: Optional[str] = None,
        default_owner: Optional[str] = None,
        default_repo: Optional[str] = None,
        auto_approve: bool = False,
        use_composio: bool = True,  # Use Composio MCP by default
        use_perplexity: bool = True  # Use Perplexity for web search
    ):
        """
        Initialize GitHub Update Automation.
        
        Args:
            analyzer: Optional GitHubEcosystemAnalyzer instance
            primary_model: Primary model to use for analysis
            secondary_model: Secondary model to use for faster operations
            use_extended_thinking: Whether to use extended thinking process for analysis
            max_thinking_tokens: Maximum number of tokens to use for thinking
            memory_path: Path to memory storage
            github_token: GitHub API token
            composio_api_key: Composio API key
            default_owner: Default repository owner
            default_repo: Default repository name
            auto_approve: Whether to auto-approve changes
            use_composio: Whether to use the Composio integration
            use_perplexity: Whether to use Perplexity for web search
        """
        self.analyzer = analyzer
        self.primary_model = primary_model
        self.secondary_model = secondary_model
        self.use_extended_thinking = use_extended_thinking
        self.max_thinking_tokens = max_thinking_tokens
        self.memory_path = memory_path or "./memory"
        self.github_token = github_token or os.environ.get("GITHUB_TOKEN")
        self.composio_api_key = composio_api_key or os.environ.get("COMPOSIO_API_KEY")
        self.default_owner = default_owner
        self.default_repo = default_repo
        self.auto_approve = auto_approve
        self.use_composio = use_composio
        self.use_perplexity = use_perplexity
        
        # Initialize memory manager if not already present
        if not hasattr(self, "memory_manager"):
            self.memory_manager = MemoryManager(memory_path=self.memory_path)
            logger.info(f"Initialized memory manager at {self.memory_path}")
            
        # Initialize MCP for advanced AI operations with maximum thinking power
        self.mcp = VotModelControlProtocol(
            primary_provider="anthropic",
            primary_model=self.primary_model,
            secondary_provider="anthropic",
            secondary_model=self.secondary_model,
            memory_manager=self.memory_manager,
            execution_mode="async",
            config={
                "use_extended_thinking": self.use_extended_thinking,
                "max_thinking_tokens": self.max_thinking_tokens
            }
        )
        logger.info(f"Initialized VOT-MCP with {self.primary_model}")
        logger.info(f"Secondary model: {self.secondary_model}")
        logger.info(f"Max thinking tokens: {self.max_thinking_tokens}")
        
        # Set up GitHub token securely (without displaying it in logs)
        if self.github_token:
            # Store token securely without revealing it
            os.environ["GITHUB_TOKEN"] = self.github_token
            masked_token = self.github_token[:4] + "..." + self.github_token[-4:] if len(self.github_token) > 8 else "***"
            logger.info(f"GitHub token configured: {masked_token}")
        else:
            logger.warning("No GitHub token provided. Limited functionality available.")
        
        # Initialize Perplexity client for web search if enabled
        if self.use_perplexity:
            try:
                from src.vot1.integrations.perplexity.search import PerplexitySearch
                self.perplexity = PerplexitySearch()
                logger.info("Initialized Perplexity Search integration")
            except Exception as e:
                logger.warning(f"Failed to initialize Perplexity Search: {e}")
                self.perplexity = None
        
        # Initialize GitHub bridge
        if self.use_composio:
            try:
                # First try using the MCP GitHub integration directly
                try:
                    # Import MCP GitHub tooling
                    logger.info("Setting up MCP GitHub integration...")
                    import json
                    from pathlib import Path
                    
                    # Load MCP configuration
                    mcp_config_path = Path("src/vot1/config/mcp.json")
                    if not mcp_config_path.exists():
                        # Try alternative paths
                        alt_paths = [
                            Path(os.getcwd()) / "src" / "vot1" / "config" / "mcp.json",
                            Path(os.path.dirname(__file__)) / ".." / "src" / "vot1" / "config" / "mcp.json"
                        ]
                        
                        for path in alt_paths:
                            if path.exists():
                                mcp_config_path = path
                                break
                        
                        # If still not found, create default config
                        if not mcp_config_path.exists():
                            config_dir = Path(os.getcwd()) / "src" / "vot1" / "config"
                            config_dir.mkdir(parents=True, exist_ok=True)
                            
                            mcp_config_path = config_dir / "mcp.json"
                            default_config = {
                                "providers": {
                                    "github": {
                                        "url": "https://mcp.composio.dev/github/victorious-damaged-branch-0ojHhf"
                                    },
                                    "perplexity": {
                                        "url": "https://mcp.composio.dev/perplexity/victorious-damaged-branch-0ojHhf"
                                    }
                                }
                            }
                            
                            with open(mcp_config_path, "w") as f:
                                json.dump(default_config, f, indent=2)
                            
                            logger.info(f"Created default MCP config at {mcp_config_path}")
                    
                    if mcp_config_path.exists():
                        with open(mcp_config_path, "r") as f:
                            mcp_config = json.load(f)
                        
                        # Check for providers configuration
                        if "providers" in mcp_config and "github" in mcp_config["providers"]:
                            github_mcp_url = mcp_config["providers"]["github"].get("url")
                            
                            if github_mcp_url:
                                # Initialize with direct MCP connection
                                logger.info(f"Using MCP GitHub integration with URL: {github_mcp_url}")
                                os.environ["MCP_GITHUB_URL"] = github_mcp_url
                                self.using_direct_mcp = True
                            else:
                                logger.warning("MCP GitHub URL not found in config")
                                self.using_direct_mcp = False
                        else:
                            logger.warning("MCP providers configuration not found")
                            self.using_direct_mcp = False
                    else:
                        logger.warning(f"MCP config not found at {mcp_config_path}")
                        self.using_direct_mcp = False
                except Exception as e:
                    logger.warning(f"Error setting up direct MCP GitHub integration: {e}")
                    self.using_direct_mcp = False
                
                # Use ComposioBridge as fallback
                self.github_bridge = GitHubComposioBridge(
                    mcp=self.mcp,
                    memory_manager=self.memory_manager,
                    default_owner=self.default_owner,
                    default_repo=self.default_repo,
                    composio_api_key=self.composio_api_key
                )
                logger.info("Using MCP Composio GitHub integration")
            except Exception as e:
                logger.warning(f"Failed to initialize Composio GitHub bridge: {e}")
                # Fallback to standard GitHub bridge
                self.github_bridge = GitHubAppBridge(
                    token=self.github_token,
                    owner=self.default_owner,
                    repo=self.default_repo
                )
                logger.info("Fallback to standard GitHub bridge")
        else:
            # Use standard GitHub bridge
            self.github_bridge = GitHubAppBridge(
                token=self.github_token,
                owner=self.default_owner,
                repo=self.default_repo
            )
            logger.info("Using standard GitHub bridge")
            
        # Initialize analyzer if not provided
        if not self.analyzer:
            logger.info("Creating new GitHub Ecosystem Analyzer")
            analyzer_args = {
                "primary_model": self.primary_model,
                "secondary_model": self.secondary_model,
                "use_extended_thinking": self.use_extended_thinking,
                "max_thinking_tokens": self.max_thinking_tokens,
                "memory_path": self.memory_path,
                "github_token": self.github_token,
                "composio_api_key": self.composio_api_key,
                "default_owner": self.default_owner,
                "default_repo": self.default_repo,
                "use_composio": self.use_composio
            }
            
            try:
                self.analyzer = GitHubEcosystemAnalyzer(**analyzer_args)
                logger.info("Successfully initialized GitHub Ecosystem Analyzer")
            except TypeError as e:
                # Handle potential TypeError if GitHubEcosystemAnalyzer interface has changed
                logger.warning(f"Error initializing analyzer with full args: {e}")
                # Try again without github_bridge argument that might be causing issues
                if "github_bridge" in str(e):
                    analyzer_args.pop("github_bridge", None)
                    self.analyzer = GitHubEcosystemAnalyzer(**analyzer_args)
                    logger.info("Successfully initialized GitHub Ecosystem Analyzer (without github_bridge)")
                else:
                    raise
                
    async def analyze_and_update(
        self,
        owner: str,
        repo: str,
        deep_analysis: bool = True,
        update_areas: List[str] = None,
        max_updates: int = 3
    ) -> Dict[str, Any]:
        """
        Analyze a repository and create updates based on analysis and improvement plan.
        
        Args:
            owner: GitHub repository owner/organization
            repo: GitHub repository name
            deep_analysis: Whether to perform deep analysis
            update_areas: List of update areas to focus on
            max_updates: Maximum number of updates to create
            
        Returns:
            Dictionary with update results
        """
        # Verify ownership for security (VillageOfThousands.io owners have special privileges)
        owners_list = ["kabrony", "villageofthousands"]
        if owner.lower() in owners_list:
            logger.info(f"Processing request from verified owner: {owner}")
            # Give priority access for owner
            self.auto_approve = True  # Auto-approve PRs for the owner
        
        logger.info(f"Starting analysis and update for {owner}/{repo}")
        
        # First analyze the repository
        analysis_result = await self.analyzer.analyze_repository(
            owner=owner,
            repo=repo,
            deep_analysis=deep_analysis
        )
        
        if not analysis_result.get("success", False) and "error" in analysis_result:
            logger.error(f"Analysis failed for {owner}/{repo}: {analysis_result.get('error')}")
            return {"error": f"Analysis failed: {analysis_result.get('error')}", "success": False}
        
        # Step 2: Generate improvement plan
        plan = await self.analyzer.generate_improvement_plan(owner, repo)
        
        if not plan.get("success", False) and "error" in plan:
            logger.error(f"Improvement plan generation failed for {owner}/{repo}: {plan.get('error')}")
            return {"error": f"Improvement plan generation failed: {plan.get('error')}", "success": False}
        
        # Step 3: Create updates based on the plan
        updates = await self.create_updates(owner, repo, analysis_result, plan, update_areas, max_updates)
        
        return {
            "repository": f"{owner}/{repo}",
            "analysis_completed": True,
            "plan_generated": True,
            "updates_created": updates,
            "success": True
        }
    
    async def create_updates(
        self,
        owner: str,
        repo: str,
        analysis: Dict[str, Any],
        plan: Dict[str, Any],
        update_areas: List[str],
        max_updates: int
    ) -> List[Dict[str, Any]]:
        """
        Create repository updates based on analysis and improvement plan.
        
        Args:
            owner: Repository owner
            repo: Repository name
            analysis: Repository analysis
            plan: Improvement plan
            update_areas: Areas to focus updates on
            max_updates: Maximum number of updates to create
            
        Returns:
            List of created updates
        """
        repo_key = f"{owner}/{repo}"
        logger.info(f"Creating updates for {repo_key} in areas: {', '.join(update_areas)}")
        
        # List to track created updates
        created_updates = []
        
        # Extract improvement areas from the plan
        improvement_plan_text = plan.get("improvement_plan", "")
        
        # Create updates for each area
        for area in update_areas:
            if len(created_updates) >= max_updates:
                logger.info(f"Reached maximum number of updates ({max_updates})")
                break
            
            if area == "documentation":
                doc_updates = await self.create_documentation_updates(owner, repo, analysis, improvement_plan_text)
                created_updates.extend(doc_updates)
            
            elif area == "workflows":
                workflow_updates = await self.create_workflow_updates(owner, repo, analysis, improvement_plan_text)
                created_updates.extend(workflow_updates)
            
            elif area == "dependencies":
                dependency_updates = await self.create_dependency_updates(owner, repo, analysis, improvement_plan_text)
                created_updates.extend(dependency_updates)
            
            elif area == "code_quality":
                code_updates = await self.create_code_quality_updates(owner, repo, analysis, improvement_plan_text)
                created_updates.extend(code_updates)
        
        logger.info(f"Created {len(created_updates)} updates for {repo_key}")
        return created_updates[:max_updates]  # Ensure we don't exceed max_updates
    
    async def create_documentation_updates(
        self,
        owner: str,
        repo: str,
        analysis: Dict[str, Any],
        improvement_plan: str
    ) -> List[Dict[str, Any]]:
        """
        Create documentation updates for a repository.
        
        Args:
            owner: Repository owner
            repo: Repository name
            analysis: Repository analysis
            improvement_plan: Improvement plan text
            
        Returns:
            List of created documentation updates
        """
        repo_key = f"{owner}/{repo}"
        logger.info(f"Creating documentation updates for {repo_key}")
        
        updates = []
        
        # Check for README.md
        has_readme = analysis.get("repository_structure", {}).get("common_files", {}).get("readme", False)
        
        if not has_readme:
            # Create a README.md
            readme_pr = await self.create_readme(owner, repo, analysis, improvement_plan)
            if readme_pr.get("success", False):
                updates.append({
                    "type": "documentation",
                    "file": "README.md",
                    "pr_number": readme_pr.get("number"),
                    "pr_url": readme_pr.get("html_url"),
                    "description": "Added a comprehensive README.md"
                })
        
        # Generate other documentation improvements based on the analysis
        doc_improvements = await self._identify_documentation_improvements(owner, repo, analysis, improvement_plan)
        
        for improvement in doc_improvements:
            doc_pr = await self.implement_documentation_improvement(owner, repo, improvement)
            if doc_pr.get("success", False):
                updates.append({
                    "type": "documentation",
                    "file": improvement.get("file"),
                    "pr_number": doc_pr.get("number"),
                    "pr_url": doc_pr.get("html_url"),
                    "description": improvement.get("description")
                })
        
        return updates
    
    async def create_readme(
        self,
        owner: str,
        repo: str,
        analysis: Dict[str, Any],
        improvement_plan: str
    ) -> Dict[str, Any]:
        """
        Create a README.md file for a repository.
        
        Args:
            owner: Repository owner
            repo: Repository name
            analysis: Repository analysis
            improvement_plan: Improvement plan text
            
        Returns:
            Pull request details
        """
        logger.info(f"Creating README.md for {owner}/{repo}")
        
        # Generate README content using the model
        prompt = f"""
        Create a comprehensive README.md for the GitHub repository {owner}/{repo}.
        
        Based on the following repository analysis:
        
        {json.dumps(analysis, indent=2)}
        
        The README should include:
        
        1. Project title and description
        2. Installation instructions
        3. Usage examples
        4. Features list
        5. Architecture overview
        6. Contributing guidelines
        7. License information
        
        Format as proper Markdown with sections and code blocks as needed.
        """
        
        readme_content = await self.mcp.process_with_optimal_model(
            prompt=prompt,
            task_complexity="medium",
            system="You are a documentation expert creating a README for a GitHub repository. Your README should be comprehensive, well-structured, and follow best practices.",
            max_tokens=2048,
            temperature=0.7
        )
        
        # Create a PR with the README
        branch_name = f"add-readme-{int(time.time())}"
        commit_message = "Add comprehensive README.md"
        pr_title = "Add comprehensive README.md"
        pr_body = "This PR adds a comprehensive README.md with installation instructions, usage examples, and project documentation."
        
        return await self._create_file_pr(
            owner=owner,
            repo=repo,
            branch_name=branch_name,
            file_path="README.md",
            file_content=readme_content,
            commit_message=commit_message,
            pr_title=pr_title,
            pr_body=pr_body
        )
    
    async def _identify_documentation_improvements(
        self,
        owner: str,
        repo: str,
        analysis: Dict[str, Any],
        improvement_plan: str
    ) -> List[Dict[str, Any]]:
        """
        Identify documentation improvements for a repository.
        
        Args:
            owner: Repository owner
            repo: Repository name
            analysis: Repository analysis
            improvement_plan: Improvement plan text
            
        Returns:
            List of documentation improvements
        """
        # Use the model to identify documentation improvements
        prompt = f"""
        Identify specific documentation improvements for the GitHub repository {owner}/{repo} based on the following analysis:
        
        {json.dumps(analysis, indent=2)}
        
        Improvement plan:
        {improvement_plan}
        
        For each improvement, specify:
        1. The file to create or update
        2. A description of the improvement
        3. The suggested content or changes
        
        Return the improvements as a JSON array of objects, each with "file", "description", and "content" fields.
        Limit to at most 3 most important documentation improvements.
        """
        
        result = await self.mcp.process_with_optimal_model(
            prompt=prompt,
            task_complexity="medium",
            system="You are a documentation expert identifying documentation improvements for a GitHub repository. Focus on high-impact improvements that would most benefit the project.",
            max_tokens=2048,
            temperature=0.7
        )
        
        # Parse the response as JSON
        try:
            if isinstance(result, str):
                # Try to extract JSON from text
                import re
                json_match = re.search(r'```json\s*([\s\S]*?)\s*```', result)
                if json_match:
                    result = json_match.group(1)
                
                return json.loads(result)
            elif isinstance(result, dict) and "content" in result:
                return json.loads(result["content"])
            elif isinstance(result, list):
                return result
            else:
                logger.warning(f"Could not parse documentation improvements result: {result}")
                return []
        except Exception as e:
            logger.error(f"Error parsing documentation improvements: {e}")
            return []
    
    async def implement_documentation_improvement(
        self,
        owner: str,
        repo: str,
        improvement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Implement a documentation improvement for a repository.
        
        Args:
            owner: Repository owner
            repo: Repository name
            improvement: Improvement details
            
        Returns:
            Pull request details
        """
        file_path = improvement.get("file")
        description = improvement.get("description")
        content = improvement.get("content")
        
        if not file_path or not content:
            return {"success": False, "error": "Missing file path or content"}
        
        logger.info(f"Implementing documentation improvement: {description} ({file_path})")
        
        # Create a PR with the documentation improvement
        branch_name = f"doc-improvement-{file_path.replace('/', '-')}-{int(time.time())}"
        commit_message = f"Update {file_path}: {description}"
        pr_title = f"Documentation: {description}"
        pr_body = f"""
        This PR improves documentation by updating `{file_path}`.
        
        ## Changes
        
        {description}
        
        ## Details
        
        This documentation improvement was automatically generated based on repository analysis.
        """
        
        return await self._create_file_pr(
            owner=owner,
            repo=repo,
            branch_name=branch_name,
            file_path=file_path,
            file_content=content,
            commit_message=commit_message,
            pr_title=pr_title,
            pr_body=pr_body
        )
    
    async def create_workflow_updates(
        self,
        owner: str,
        repo: str,
        analysis: Dict[str, Any],
        improvement_plan: str
    ) -> List[Dict[str, Any]]:
        """
        Create GitHub Actions workflow updates for a repository.
        
        Args:
            owner: Repository owner
            repo: Repository name
            analysis: Repository analysis
            improvement_plan: Improvement plan text
            
        Returns:
            List of created workflow updates
        """
        repo_key = f"{owner}/{repo}"
        logger.info(f"Creating workflow updates for {repo_key}")
        
        updates = []
        
        # Check for .github/workflows directory
        has_workflows = analysis.get("repository_structure", {}).get("common_files", {}).get("github_directory", False)
        
        # Generate workflow improvements
        workflow_improvements = await self._identify_workflow_improvements(owner, repo, analysis, improvement_plan)
        
        for improvement in workflow_improvements:
            workflow_pr = await self.implement_workflow_improvement(owner, repo, improvement)
            if workflow_pr.get("success", False):
                updates.append({
                    "type": "workflow",
                    "file": improvement.get("file"),
                    "pr_number": workflow_pr.get("number"),
                    "pr_url": workflow_pr.get("html_url"),
                    "description": improvement.get("description")
                })
        
        return updates
    
    async def _identify_workflow_improvements(
        self,
        owner: str,
        repo: str,
        analysis: Dict[str, Any],
        improvement_plan: str
    ) -> List[Dict[str, Any]]:
        """
        Identify GitHub Actions workflow improvements for a repository.
        
        Args:
            owner: Repository owner
            repo: Repository name
            analysis: Repository analysis
            improvement_plan: Improvement plan text
            
        Returns:
            List of workflow improvements
        """
        # Use the model to identify workflow improvements
        prompt = f"""
        Identify GitHub Actions workflow improvements for the repository {owner}/{repo} based on the following analysis:
        
        {json.dumps(analysis, indent=2)}
        
        Improvement plan:
        {improvement_plan}
        
        For each workflow improvement, specify:
        1. The workflow file path (e.g., .github/workflows/ci.yml)
        2. A description of the workflow
        3. The complete workflow file content in YAML format
        
        Return the improvements as a JSON array of objects, each with "file", "description", and "content" fields.
        Limit to at most 2 most important workflow improvements.
        """
        
        result = await self.mcp.process_with_optimal_model(
            prompt=prompt,
            task_complexity="complex",
            system="You are a DevOps expert creating GitHub Actions workflows. Your workflows should follow best practices and be efficient, reliable, and secure.",
            max_tokens=3072,
            temperature=0.7
        )
        
        # Parse the response as JSON
        try:
            if isinstance(result, str):
                # Try to extract JSON from text
                import re
                json_match = re.search(r'```json\s*([\s\S]*?)\s*```', result)
                if json_match:
                    result = json_match.group(1)
                
                return json.loads(result)
            elif isinstance(result, dict) and "content" in result:
                return json.loads(result["content"])
            elif isinstance(result, list):
                return result
            else:
                logger.warning(f"Could not parse workflow improvements result: {result}")
                return []
        except Exception as e:
            logger.error(f"Error parsing workflow improvements: {e}")
            return []
    
    async def implement_workflow_improvement(
        self,
        owner: str,
        repo: str,
        improvement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Implement a GitHub Actions workflow improvement for a repository.
        
        Args:
            owner: Repository owner
            repo: Repository name
            improvement: Improvement details
            
        Returns:
            Pull request details
        """
        file_path = improvement.get("file")
        description = improvement.get("description")
        content = improvement.get("content")
        
        if not file_path or not content:
            return {"success": False, "error": "Missing file path or content"}
        
        logger.info(f"Implementing workflow improvement: {description} ({file_path})")
        
        # Create a PR with the workflow improvement
        branch_name = f"workflow-improvement-{int(time.time())}"
        commit_message = f"Add GitHub Actions workflow: {description}"
        pr_title = f"CI/CD: {description}"
        pr_body = f"""
        This PR adds a GitHub Actions workflow: `{file_path}`.
        
        ## Workflow Purpose
        
        {description}
        
        ## Details
        
        This workflow was automatically generated based on repository analysis and best practices.
        """
        
        return await self._create_file_pr(
            owner=owner,
            repo=repo,
            branch_name=branch_name,
            file_path=file_path,
            file_content=content,
            commit_message=commit_message,
            pr_title=pr_title,
            pr_body=pr_body
        )
    
    async def create_dependency_updates(
        self,
        owner: str,
        repo: str,
        analysis: Dict[str, Any],
        improvement_plan: str
    ) -> List[Dict[str, Any]]:
        """
        Create dependency updates for a repository.
        
        Args:
            owner: Repository owner
            repo: Repository name
            analysis: Repository analysis
            improvement_plan: Improvement plan text
            
        Returns:
            List of created dependency updates
        """
        repo_key = f"{owner}/{repo}"
        logger.info(f"Creating dependency updates for {repo_key}")
        
        updates = []
        
        # Generate dependency improvements
        dependency_improvements = await self._identify_dependency_improvements(owner, repo, analysis, improvement_plan)
        
        for improvement in dependency_improvements:
            dependency_pr = await self.implement_dependency_improvement(owner, repo, improvement)
            if dependency_pr.get("success", False):
                updates.append({
                    "type": "dependency",
                    "file": improvement.get("file"),
                    "pr_number": dependency_pr.get("number"),
                    "pr_url": dependency_pr.get("html_url"),
                    "description": improvement.get("description")
                })
        
        return updates
    
    async def _identify_dependency_improvements(
        self,
        owner: str,
        repo: str,
        analysis: Dict[str, Any],
        improvement_plan: str
    ) -> List[Dict[str, Any]]:
        """
        Identify dependency improvements for a repository.
        
        Args:
            owner: Repository owner
            repo: Repository name
            analysis: Repository analysis
            improvement_plan: Improvement plan text
            
        Returns:
            List of dependency improvements
        """
        # Use the model to identify dependency improvements
        prompt = f"""
        Identify dependency improvements for the repository {owner}/{repo} based on the following analysis:
        
        {json.dumps(analysis, indent=2)}
        
        Improvement plan:
        {improvement_plan}
        
        For each dependency improvement, specify:
        1. The dependency file path (e.g., requirements.txt, package.json)
        2. A description of the improvement
        3. The updated file content
        
        Return the improvements as a JSON array of objects, each with "file", "description", and "content" fields.
        Limit to at most 1 most important dependency improvement.
        """
        
        result = await self.mcp.process_with_optimal_model(
            prompt=prompt,
            task_complexity="medium",
            system="You are a dependency management expert updating dependencies for a GitHub repository. Focus on security updates, performance improvements, and keeping dependencies up-to-date.",
            max_tokens=2048,
            temperature=0.7
        )
        
        # Parse the response as JSON
        try:
            if isinstance(result, str):
                # Try to extract JSON from text
                import re
                json_match = re.search(r'```json\s*([\s\S]*?)\s*```', result)
                if json_match:
                    result = json_match.group(1)
                
                return json.loads(result)
            elif isinstance(result, dict) and "content" in result:
                return json.loads(result["content"])
            elif isinstance(result, list):
                return result
            else:
                logger.warning(f"Could not parse dependency improvements result: {result}")
                return []
        except Exception as e:
            logger.error(f"Error parsing dependency improvements: {e}")
            return []
    
    async def implement_dependency_improvement(
        self,
        owner: str,
        repo: str,
        improvement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Implement a dependency improvement for a repository.
        
        Args:
            owner: Repository owner
            repo: Repository name
            improvement: Improvement details
            
        Returns:
            Pull request details
        """
        file_path = improvement.get("file")
        description = improvement.get("description")
        content = improvement.get("content")
        
        if not file_path or not content:
            return {"success": False, "error": "Missing file path or content"}
        
        logger.info(f"Implementing dependency improvement: {description} ({file_path})")
        
        # Create a PR with the dependency improvement
        branch_name = f"dependency-update-{int(time.time())}"
        commit_message = f"Update dependencies in {file_path}"
        pr_title = f"Dependencies: {description}"
        pr_body = f"""
        This PR updates dependencies in `{file_path}`.
        
        ## Changes
        
        {description}
        
        ## Details
        
        This dependency update was automatically generated based on repository analysis.
        """
        
        return await self._create_file_pr(
            owner=owner,
            repo=repo,
            branch_name=branch_name,
            file_path=file_path,
            file_content=content,
            commit_message=commit_message,
            pr_title=pr_title,
            pr_body=pr_body
        )
    
    async def create_code_quality_updates(
        self,
        owner: str,
        repo: str,
        analysis: Dict[str, Any],
        improvement_plan: str
    ) -> List[Dict[str, Any]]:
        """
        Create code quality updates for a repository.
        
        Args:
            owner: Repository owner
            repo: Repository name
            analysis: Repository analysis
            improvement_plan: Improvement plan text
            
        Returns:
            List of created code quality updates
        """
        repo_key = f"{owner}/{repo}"
        logger.info(f"Creating code quality updates for {repo_key}")
        
        updates = []
        
        # Generate code quality improvements
        code_improvements = await self._identify_code_quality_improvements(owner, repo, analysis, improvement_plan)
        
        for improvement in code_improvements:
            code_pr = await self.implement_code_quality_improvement(owner, repo, improvement)
            if code_pr.get("success", False):
                updates.append({
                    "type": "code_quality",
                    "file": improvement.get("file"),
                    "pr_number": code_pr.get("number"),
                    "pr_url": code_pr.get("html_url"),
                    "description": improvement.get("description")
                })
        
        return updates
    
    async def _identify_code_quality_improvements(
        self,
        owner: str,
        repo: str,
        analysis: Dict[str, Any],
        improvement_plan: str
    ) -> List[Dict[str, Any]]:
        """
        Identify code quality improvements for a repository.
        
        Args:
            owner: Repository owner
            repo: Repository name
            analysis: Repository analysis
            improvement_plan: Improvement plan text
            
        Returns:
            List of code quality improvements
        """
        # Use the model to identify code quality improvements
        prompt = f"""
        Identify code quality improvements for the repository {owner}/{repo} based on the following analysis:
        
        {json.dumps(analysis, indent=2)}
        
        Improvement plan:
        {improvement_plan}
        
        For each code quality improvement, specify:
        1. The file path to improve
        2. A description of the improvement
        3. The suggested changes or new content
        
        Return the improvements as a JSON array of objects, each with "file", "description", and "content" fields.
        Focus on adding linter configurations, code style standardization, or other code quality tooling.
        Limit to at most 1 most important code quality improvement.
        """
        
        result = await self.mcp.process_with_optimal_model(
            prompt=prompt,
            task_complexity="complex",
            system="You are a code quality expert improving a GitHub repository. Focus on adding tools and configurations that will help maintain code quality over time.",
            max_tokens=2048,
            temperature=0.7
        )
        
        # Parse the response as JSON
        try:
            if isinstance(result, str):
                # Try to extract JSON from text
                import re
                json_match = re.search(r'```json\s*([\s\S]*?)\s*```', result)
                if json_match:
                    result = json_match.group(1)
                
                return json.loads(result)
            elif isinstance(result, dict) and "content" in result:
                return json.loads(result["content"])
            elif isinstance(result, list):
                return result
            else:
                logger.warning(f"Could not parse code quality improvements result: {result}")
                return []
        except Exception as e:
            logger.error(f"Error parsing code quality improvements: {e}")
            return []
    
    async def implement_code_quality_improvement(
        self,
        owner: str,
        repo: str,
        improvement: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Implement a code quality improvement for a repository.
        
        Args:
            owner: Repository owner
            repo: Repository name
            improvement: Improvement details
            
        Returns:
            Pull request details
        """
        file_path = improvement.get("file")
        description = improvement.get("description")
        content = improvement.get("content")
        
        if not file_path or not content:
            return {"success": False, "error": "Missing file path or content"}
        
        logger.info(f"Implementing code quality improvement: {description} ({file_path})")
        
        # Create a PR with the code quality improvement
        branch_name = f"code-quality-{int(time.time())}"
        commit_message = f"Add code quality improvement: {description}"
        pr_title = f"Code Quality: {description}"
        pr_body = f"""
        This PR improves code quality by adding `{file_path}`.
        
        ## Purpose
        
        {description}
        
        ## Details
        
        This code quality improvement was automatically generated based on repository analysis.
        """
        
        return await self._create_file_pr(
            owner=owner,
            repo=repo,
            branch_name=branch_name,
            file_path=file_path,
            file_content=content,
            commit_message=commit_message,
            pr_title=pr_title,
            pr_body=pr_body
        )
    
    async def _create_file_pr(
        self,
        owner: str,
        repo: str,
        branch_name: str,
        file_path: str,
        file_content: str,
        commit_message: str,
        pr_title: str,
        pr_body: str
    ) -> Dict[str, Any]:
        """
        Create a pull request that adds or updates a file.
        
        Args:
            owner: Repository owner
            repo: Repository name
            branch_name: New branch name
            file_path: Path to the file to add/update
            file_content: File content
            commit_message: Commit message
            pr_title: Pull request title
            pr_body: Pull request body
            
        Returns:
            Pull request details
        """
        try:
            # Create a new branch
            await self.github_bridge.create_branch(owner, repo, branch_name)
            
            # Create or update the file
            file_result = await self.github_bridge.create_or_update_file(
                owner=owner,
                repo=repo,
                path=file_path,
                content=file_content,
                message=commit_message,
                branch=branch_name
            )
            
            if not file_result.get("success", False):
                logger.error(f"Failed to create/update file: {file_result}")
                return {"success": False, "error": f"Failed to create/update file: {file_result.get('error')}"}
            
            # Create a pull request
            pr_result = await self.github_bridge.create_pull_request(
                owner=owner,
                repo=repo,
                title=pr_title,
                body=pr_body,
                head=branch_name,
                base="main"  # Assuming main is the default branch
            )
            
            if not pr_result.get("success", False):
                logger.error(f"Failed to create pull request: {pr_result}")
                return {"success": False, "error": f"Failed to create pull request: {pr_result.get('error')}"}
            
            logger.info(f"Created pull request #{pr_result.get('number')}: {pr_result.get('html_url')}")
            
            # Auto-approve if enabled
            if self.auto_approve:
                await self.github_bridge.approve_pull_request(
                    owner=owner,
                    repo=repo,
                    pr_number=pr_result.get("number")
                )
                logger.info(f"Auto-approved pull request #{pr_result.get('number')}")
            
            return {
                "success": True,
                "number": pr_result.get("number"),
                "html_url": pr_result.get("html_url"),
                "id": pr_result.get("id")
            }
            
        except Exception as e:
            logger.error(f"Error creating PR: {e}")
            return {"success": False, "error": str(e)}

    async def web_search(self, query: str) -> Dict[str, Any]:
        """
        Perform a web search using Perplexity to enhance GitHub analysis.
        
        Args:
            query: Search query
            
        Returns:
            Dictionary with search results
        """
        if hasattr(self, "perplexity") and self.perplexity:
            try:
                logger.info(f"Performing web search: {query}")
                results = await self.perplexity.search(query)
                return results
            except Exception as e:
                logger.warning(f"Error performing web search: {e}")
                return {"error": str(e), "results": []}
        else:
            logger.warning("Perplexity search not available")
            return {"error": "Perplexity search not available", "results": []}

    async def create_webhook(self, owner: str, repo: str, webhook_url: str, 
                        events: Optional[List[str]] = None, secret: Optional[str] = None) -> Dict[str, Any]:
        """
        Create a webhook for a repository.
        
        Args:
            owner: Repository owner
            repo: Repository name
            webhook_url: Webhook URL to receive events
            events: List of events to trigger the webhook (default: ["push", "pull_request"])
            secret: Secret for webhook signature verification
            
        Returns:
            Webhook creation result
        """
        try:
            if events is None:
                events = ["push", "pull_request"]
                
            logger.info(f"Creating webhook for {owner}/{repo} with events: {events}")
            
            payload = {
                "name": "web",
                "active": True,
                "events": events,
                "config": {
                    "url": webhook_url,
                    "content_type": "json"
                }
            }
            
            if secret:
                payload["config"]["secret"] = secret
                
            result = await self.github_bridge.create_webhook(owner, repo, payload)
            logger.info(f"Webhook created successfully for {owner}/{repo}")
            return result
        except Exception as e:
            logger.error(f"Error creating webhook: {e}")
            return {"success": False, "error": str(e)}
    
    async def list_workflows(self, owner: str, repo: str) -> Dict[str, Any]:
        """
        List GitHub Actions workflows in a repository.
        
        Args:
            owner: Repository owner
            repo: Repository name
            
        Returns:
            Dictionary containing workflow information
        """
        try:
            logger.info(f"Listing workflows for {owner}/{repo}")
            result = await self.github_bridge.list_workflows(owner, repo)
            logger.info(f"Successfully retrieved {len(result.get('workflows', []))} workflows from {owner}/{repo}")
            return result
        except Exception as e:
            logger.error(f"Error listing workflows: {e}")
            return {"success": False, "error": str(e)}
    
    async def create_or_update_workflow(self, owner: str, repo: str, workflow_path: str, 
                                        workflow_content: str, commit_message: str = "Update workflow") -> Dict[str, Any]:
        """
        Create or update a GitHub Actions workflow file.
        
        Args:
            owner: Repository owner
            repo: Repository name
            workflow_path: Path to the workflow file (e.g., ".github/workflows/ci.yml")
            workflow_content: Content of the workflow file
            commit_message: Commit message for the update
            
        Returns:
            Dictionary containing the result of the operation
        """
        logger.info(f"Creating/updating workflow at {workflow_path} for {owner}/{repo}")
        
        try:
            # Make sure the directory exists
            directory = os.path.dirname(workflow_path)
            if directory:
                # Check if the directory already exists
                if not await self._directory_exists(owner, repo, directory):
                    # Create the directory structure if needed
                    logger.info(f"Creating {directory} directory in {owner}/{repo}")
                    dir_result = await self._create_directory_if_needed(owner, repo, directory)
                    if not dir_result.get("success"):
                        return {"success": False, "error": f"Error creating directory {directory}: {dir_result.get('error')}"}
            
            # Create or update the workflow file
            result = await self.github_bridge.create_or_update_file(
                owner=owner,
                repo=repo,
                path=workflow_path,
                content=workflow_content,
                message=commit_message,
                branch="main"
            )
            
            if result.get("success"):
                logger.info(f"Successfully created/updated workflow at {workflow_path}")
                return {"success": True, "workflow": result.get("content", {})}
            else:
                logger.error(f"Error creating/updating workflow: {result.get('error')}")
                return {"success": False, "error": result.get("error")}
        except Exception as e:
            logger.error(f"Error creating/updating workflow: {e}")
            return {"success": False, "error": str(e)}
    
    async def trigger_workflow(self, owner: str, repo: str, workflow_id: str, 
                              ref: str = "main", inputs: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Trigger a GitHub Actions workflow.
        
        Args:
            owner: Repository owner
            repo: Repository name
            workflow_id: Workflow ID or filename
            ref: The reference (branch, tag, commit) to run the workflow on
            inputs: Optional inputs for the workflow
            
        Returns:
            Dictionary containing the result of the operation
        """
        try:
            logger.info(f"Triggering workflow {workflow_id} for {owner}/{repo} on ref {ref}")
            
            payload = {
                "ref": ref
            }
            
            if inputs:
                payload["inputs"] = inputs
            
            result = await self.github_bridge.dispatch_workflow(owner, repo, workflow_id, payload)
            logger.info(f"Workflow {workflow_id} triggered successfully for {owner}/{repo}")
            return result
        except Exception as e:
            logger.error(f"Error triggering workflow: {e}")
            return {"success": False, "error": str(e)}
    
    async def _directory_exists(self, owner: str, repo: str, directory_path: str) -> bool:
        """Check if a directory exists in the repository."""
        try:
            logger.info(f"Checking if directory {directory_path} exists in {owner}/{repo}")
            # Try to fetch the directory contents to see if it exists
            result = await self.github_bridge._make_github_api_request("GET", f"repos/{owner}/{repo}/contents/{directory_path}")
            return isinstance(result, list) or (isinstance(result, dict) and result.get("type") == "dir")
        except Exception as e:
            logger.warning(f"Error checking if directory {directory_path} exists: {e}")
            return False
    
    async def _create_directory_if_needed(self, owner: str, repo: str, directory_path: str) -> Dict[str, Any]:
        """Create a directory in the repository if it doesn't exist."""
        try:
            logger.info(f"Creating {directory_path} directory in {owner}/{repo}")
            
            # Check if the directory already exists
            if await self._directory_exists(owner, repo, directory_path):
                logger.info(f"Directory {directory_path} already exists")
                return {"success": True}
            
            # Create the directory by adding a .gitkeep file
            result = await self.github_bridge.create_or_update_file(
                owner=owner,
                repo=repo,
                path=f"{directory_path}/.gitkeep",
                content="# This file is used to create the directory structure\n",
                message=f"Create {directory_path} directory",
                branch="main"
            )
            
            if result.get("success"):
                logger.info(f"Created {directory_path} directory in {owner}/{repo}")
                return {"success": True}
            else:
                logger.error(f"Error creating directory {directory_path}: {result.get('error')}")
                return {"success": False, "error": result.get("error")}
        except Exception as e:
            logger.error(f"Error creating directory {directory_path}: {e}")
            return {"success": False, "error": str(e)}

async def main():
    """Run the GitHub Update Automation."""
    parser = argparse.ArgumentParser(description="GitHub Update Automation")
    parser.add_argument("--owner", help="GitHub repository owner")
    parser.add_argument("--repo", help="GitHub repository name")
    parser.add_argument("--repos-file", help="JSON file containing multiple repositories to update")
    parser.add_argument("--deep-analysis", action="store_true", help="Perform deep analysis with the primary model")
    parser.add_argument("--update-areas", nargs='+', choices=["documentation", "workflows", "dependencies", "code_quality"], default=["documentation", "workflows"], help="Areas to update")
    parser.add_argument("--max-updates", type=int, default=3, help="Maximum number of updates to create")
    parser.add_argument("--memory-path", help="Path to memory storage")
    parser.add_argument("--use-composio", action="store_true", help="Use Composio for enhanced GitHub integration")
    parser.add_argument("--auto-approve", action="store_true", help="Auto-approve created PRs")
    parser.add_argument("--max-thinking-tokens", type=int, default=8000, help="Maximum thinking tokens")
    parser.add_argument("--disable-extended-thinking", action="store_true", help="Disable extended thinking for deep analysis")
    
    args = parser.parse_args()
    
    # Initialize the update automation
    updater = GitHubUpdateAutomation(
        use_extended_thinking=not args.disable_extended_thinking,
        max_thinking_tokens=args.max_thinking_tokens,
        memory_path=args.memory_path,
        default_owner=args.owner,
        default_repo=args.repo,
        use_composio=args.use_composio,
        auto_approve=args.auto_approve
    )
    
    result = None
    
    # Process based on arguments
    if args.repos_file:
        # Update multiple repositories
        with open(args.repos_file, 'r') as f:
            repos = json.load(f)
        
        results = []
        for repo_info in repos:
            owner = repo_info.get("owner")
            repo = repo_info.get("repo")
            if owner and repo:
                result = await updater.analyze_and_update(
                    owner=owner,
                    repo=repo,
                    deep_analysis=args.deep_analysis,
                    update_areas=args.update_areas,
                    max_updates=args.max_updates
                )
                results.append(result)
        
        print(json.dumps(results, indent=2))
    else:
        # Update a single repository
        result = await updater.analyze_and_update(
            owner=args.owner,
            repo=args.repo,
            deep_analysis=args.deep_analysis,
            update_areas=args.update_areas,
            max_updates=args.max_updates
        )
        print(json.dumps(result, indent=2))

if __name__ == "__main__":
    asyncio.run(main()) 